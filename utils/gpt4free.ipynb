{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f390cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g4f.client import Client\n",
    "import g4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbda7430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В Python вы можете использовать модуль `subprocess` для запуска процесса командной строки. Чтобы запустить процесс и не ожидать его завершения, вы можете использовать метод `Popen`. Для того чтобы процесс завершался при завершении основной программы, вы можете использовать метод `terminate()` в блоке `finally` или использовать `atexit` для регистрации функции завершения.\n",
      "\n",
      "Вот пример кода, который демонстрирует, как это сделать:\n",
      "\n",
      "```python\n",
      "import subprocess\n",
      "import atexit\n",
      "\n",
      "# Запускаем процесс\n",
      "process = subprocess.Popen(['python', '-m', 'g4f.api.run'])\n",
      "\n",
      "# Функция для завершения процесса\n",
      "def cleanup():\n",
      "    process.terminate()\n",
      "\n",
      "# Регистрируем функцию завершения\n",
      "atexit.register(cleanup)\n",
      "\n",
      "# Основная программа\n",
      "try:\n",
      "    # Ваш код здесь\n",
      "    print(\"Основная программа работает...\")\n",
      "    # Например, можно добавить задержку\n",
      "    import time\n",
      "    time.sleep(10)  # Замените на вашу логику\n",
      "finally:\n",
      "    cleanup()  # Убедитесь, что процесс завершен\n",
      "```\n",
      "\n",
      "В этом коде:\n",
      "\n",
      "1. Мы используем `subprocess.Popen` для запуска команды.\n",
      "2. Регистрируем функцию `cleanup`, которая завершает процесс, с помощью `atexit.register`.\n",
      "3. В блоке `try` вы можете разместить вашу основную логику программы.\n",
      "4. В блоке `finally` мы вызываем `cleanup`, чтобы гарантировать, что процесс будет завершен, когда основная программа завершится. \n",
      "\n",
      "Таким образом, процесс будет запущен, и вы сможете продолжать выполнение основной программы, не дожидаясь завершения запущенного процесса.\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "response = client.chat.completions.create(\n",
    "    # model=\"gpt-3.5-turbo\", gpt-4o-mini\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # model=\"gpt-4-turbo\",\n",
    "    # model=\"gpt-3.5-turbo-16k\",\n",
    "    # model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Как в Python (windows 11) запустить процесс командной строки (`python -m g4f.api.run`), но не ожидать его завершения? При этом процесс должен завершаться при завершении основной программы.\"}],\n",
    "    # Add any other necessary parameters\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661f1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airoboros-70b',\n",
       " 'aria',\n",
       " 'blackboxai',\n",
       " 'codegemma-7b',\n",
       " 'command-a',\n",
       " 'command-r',\n",
       " 'command-r7b',\n",
       " 'command-r-plus',\n",
       " 'dall-e-3',\n",
       " 'deepseek-prover-v2',\n",
       " 'deepseek-prover-v2-671b',\n",
       " 'deepseek-r1',\n",
       " 'deepseek-r1-0528',\n",
       " 'deepseek-r1-0528-turbo',\n",
       " 'deepseek-r1-distill-llama-70b',\n",
       " 'deepseek-r1-distill-qwen-14b',\n",
       " 'deepseek-r1-distill-qwen-1.5b',\n",
       " 'deepseek-r1-distill-qwen-32b',\n",
       " 'deepseek-r1-turbo',\n",
       " 'deepseek-v3',\n",
       " 'deepseek-v3-0324',\n",
       " 'deepseek-v3-0324-turbo',\n",
       " 'dolphin-2.6',\n",
       " 'dolphin-2.9',\n",
       " 'evil',\n",
       " 'flux',\n",
       " 'flux-canny',\n",
       " 'flux-depth',\n",
       " 'flux-dev',\n",
       " 'flux-dev-lora',\n",
       " 'flux-kontext-dev',\n",
       " 'flux-kontext-max',\n",
       " 'flux-kontext-pro',\n",
       " 'flux-pro',\n",
       " 'flux-redux',\n",
       " 'flux-schnell',\n",
       " 'gemini-2.0',\n",
       " 'gemini-1.5-flash',\n",
       " 'gemini-1.5-pro',\n",
       " 'gemini-2.0-flash',\n",
       " 'gemini-2.0-flash-thinking',\n",
       " 'gemini-2.0-flash-thinking-with-apps',\n",
       " 'gemini-2.5-flash',\n",
       " 'gemini-2.5-pro',\n",
       " 'gemma-1.1-7b',\n",
       " 'gemma-2-27b',\n",
       " 'gemma-2-9b',\n",
       " 'gemma-2b',\n",
       " 'gemma-3-12b',\n",
       " 'gemma-3-27b',\n",
       " 'gemma-3-4b',\n",
       " 'gemma-3n-e4b',\n",
       " 'gpt-4',\n",
       " 'gpt-4.1',\n",
       " 'gpt-4.1-mini',\n",
       " 'gpt-4.1-nano',\n",
       " 'gpt-4.5',\n",
       " 'gpt-4o',\n",
       " 'gpt-4o-mini',\n",
       " 'gpt-4o-mini-audio',\n",
       " 'gpt-4o-mini-tts',\n",
       " 'gpt-image',\n",
       " 'grok-2',\n",
       " 'grok-3',\n",
       " 'grok-3-mini',\n",
       " 'grok-3-r1',\n",
       " 'hermes-2-dpo',\n",
       " 'janus-pro-7b',\n",
       " 'lfm-40b',\n",
       " 'llama-2-70b',\n",
       " 'llama-2-7b',\n",
       " 'llama-3.1-405b',\n",
       " 'llama-3.1-70b',\n",
       " 'llama-3.1-8b',\n",
       " 'llama-3.2-11b',\n",
       " 'llama-3.2-1b',\n",
       " 'llama-3.2-3b',\n",
       " 'llama-3.2-90b',\n",
       " 'llama-3.3-70b',\n",
       " 'llama-3-70b',\n",
       " 'llama-3-8b',\n",
       " 'llama-4-maverick',\n",
       " 'llama-4-scout',\n",
       " 'lzlv-70b',\n",
       " 'meta-ai',\n",
       " 'mistral-7b',\n",
       " 'mistral-nemo',\n",
       " 'mistral-small-24b',\n",
       " 'mistral-small-3.1-24b',\n",
       " 'mixtral-8x7b',\n",
       " 'nemotron-70b',\n",
       " 'o1',\n",
       " 'o1-mini',\n",
       " 'o3-mini',\n",
       " 'o3-mini-high',\n",
       " 'o4-mini',\n",
       " 'o4-mini-high',\n",
       " 'phi-3.5-mini',\n",
       " 'phi-4',\n",
       " 'phi-4-multimodal',\n",
       " 'phi-4-reasoning-plus',\n",
       " 'qwen-1.5-7b',\n",
       " 'qwen-2.5',\n",
       " 'qwen-2.5-1m',\n",
       " 'qwen-2.5-72b',\n",
       " 'qwen-2.5-7b',\n",
       " 'qwen-2.5-coder-32b',\n",
       " 'qwen-2.5-max',\n",
       " 'qwen-2.5-vl-72b',\n",
       " 'qwen-2-72b',\n",
       " 'qwen-2-vl-72b',\n",
       " 'qwen-2-vl-7b',\n",
       " 'qwen-3-0.6b',\n",
       " 'qwen-3-14b',\n",
       " 'qwen-3-1.7b',\n",
       " 'qwen-3-235b',\n",
       " 'qwen-3-30b',\n",
       " 'qwen-3-32b',\n",
       " 'qwen-3-4b',\n",
       " 'qwq-32b',\n",
       " 'r1-1776',\n",
       " 'sd-3.5-large',\n",
       " 'sdxl-turbo',\n",
       " 'sonar',\n",
       " 'sonar-pro',\n",
       " 'sonar-reasoning',\n",
       " 'sonar-reasoning-pro',\n",
       " 'wizardlm-2-7b',\n",
       " 'wizardlm-2-8x22b']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = [el for el in dir(g4f.models) if not el.startswith(\"_\")]\n",
    "available_models = [\n",
    "    getattr(g4f.models, model).name\n",
    "    for model in available_models\n",
    "    if hasattr(getattr(g4f.models, model), \"name\")\n",
    "]\n",
    "# 'deepseek-v3',\n",
    "# 'deepseek-v3-0324',\n",
    "# 'deepseek-v3-0324-turbo',\n",
    "# 'gemini-2.0-flash',\n",
    "# 'gemini-2.0-flash-thinking',\n",
    "# 'gemini-2.0-flash-thinking-with-apps',\n",
    "# 'gemini-2.5-flash',\n",
    "# 'gemini-2.5-pro',\n",
    "# 'o1',\n",
    "# 'o1-mini',\n",
    "# 'o3-mini',\n",
    "# 'o3-mini-high',\n",
    "# 'o4-mini',\n",
    "# 'o4-mini-high',\n",
    "# 'gpt-4',\n",
    "# 'gpt-4.1',\n",
    "# 'gpt-4.1-mini',\n",
    "# 'gpt-4.1-nano',\n",
    "# 'gpt-4.5',\n",
    "# 'gpt-4o',\n",
    "# 'gpt-4o-mini',\n",
    "# \n",
    "available_models = [el for el in available_models if len(el) > 0]\n",
    "available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77baaa5",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39211213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запуск сервера\n",
    "!python -m g4f.api.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d1d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # Optional: Set your Hugging Face token for embeddings\n",
    "    api_key=\"YOUR_HUGGING_FACE_TOKEN\",\n",
    "    base_url=\"http://localhost:1337/v1\"\n",
    ")\n",
    "\n",
    "# Use it like the official OpenAI client\n",
    "# chat_completion = client.chat.completions.create(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Write a poem about a tree\"}],\n",
    "#     stream=True,\n",
    "# )\n",
    "\n",
    "# for token in chat_completion:\n",
    "#     content = token.choices[0].delta.content\n",
    "#     if content is not None:\n",
    "#         print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7188171",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a poem about a tree\"}],\n",
    "    stream=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea8b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In a quiet glade where the sunlight weaves,  \\nStands a gentle giant with whispered leaves.  \\nArms outstretched in a regal pose,  \\nA sentinel of secrets, the elder tree knows.  \\n\\nRoots entwined in the earth below,  \\nA tapestry woven with ages aglow.  \\nWith stories in rings that tell of the years,  \\nOf laughter and sorrow, of joy and of tears.  \\n\\nIn spring's soft embrace, with blossoms of white,  \\nIt dances in breezes, a marvelous sight.  \\nBirds find their refuge in its leafy embrace,  \\nAs shadows and sunlight weave through the grace.  \\n\\nSummer brings warmth, and with each golden ray,  \\nChildren find laughter in its shade at play.  \\nA fortress of dreams, where imaginations thrive,  \\nIn the arms of this tree, all wonders come alive.  \\n\\nWhen autumn arrives with her palette of gold,  \\nThe leaves whirl like memories, stories retold.  \\nA carpet of red, orange, and brown,  \\nThe tree stands in majesty, wearing its crown.  \\n\\nThen winter arrives, with her blanket of white,  \\nThe branches stand bare, yet still hold the light.  \\nIn silence it slumbers, awaiting the spring,  \\nA promise of life, of new hopeful things.  \\n\\nOh tree of the ages, with wisdom so deep,  \\nYour roots in the earth, your branches in dreams,  \\nYou witness the world in its ebbs and its flows,  \\nA timeless reminder that life ever grows.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
